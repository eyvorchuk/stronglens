{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import h5py\n",
    "import numpy as np\n",
    "from astropy.nddata.utils import Cutout2D\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.visualization import (ZScaleInterval, ImageNormalize)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.utils import shuffle\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create hdf5 files for lenses and non-lenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"/home/eyvorch9/projects/rrg-kyi/astro/cfis/W3/\"\n",
    "label_dir = \"/home/eyvorch9/scratch/labels/\"\n",
    "label_subdir = \"stronglensdb_confirmed_unige/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the hdf5 files already exist\n",
    "cutout_dir = os.path.expandvars(\"$SLURM_TMPDIR\") + \"/\"\n",
    "hf_pos = h5py.File(cutout_dir + \"labelled_cutouts_alt.h5\", \"r+\")\n",
    "hf_neg = h5py.File(cutout_dir + \"random_cutouts_cfis.h5\", \"r+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the hdf5 files do not exist\n",
    "src = os.path.expandvars(\"$SCRATCH\") + \"/\"\n",
    "hf_neg = h5py.File(src + \"random_cutouts_cfis.h5\", \"w\")\n",
    "hf_neg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = os.path.expandvars(\"$SCRATCH\") + \"/random_cutouts_cfis.h5\"\n",
    "dest = os.path.expandvars(\"$SLURM_TMPDIR\") + \"/\"\n",
    "shutil.copy2(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_neg = h5py.File(dest + \"random_cutouts_cfis.h5\", \"r+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_list = open(image_dir + \"tiles.list\", \"r\")\n",
    "tile_files = tile_list.readlines()\n",
    "for i in range(len(tile_files)):\n",
    "    tile_files[i] = tile_files[i][:-1]\n",
    "    print(tile_files[i])\n",
    "tile_list.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_id = \"157.275\"\n",
    "shutil.copy2(image_dir + f\"CFIS.{tile_id}.u.fits\", cutout_dir)\n",
    "#shutil.copy2(image_dir + f\"PS1.{tile_id}.g.fits\", cutout_dir)\n",
    "shutil.copy2(image_dir + f\"CFIS.{tile_id}.r.fits\", cutout_dir)\n",
    "#shutil.copy2(image_dir + f\"PS1.{tile_id}.i.fits\", cutout_dir)\n",
    "#shutil.copy2(image_dir + f\"PS1.{tile_id}.z.fits\", cutout_dir)\n",
    "shutil.copy2(image_dir + f\"CFIS.{tile_id}.r.cat\", cutout_dir)\n",
    "u_image = cutout_dir + f\"CFIS.{tile_id}.u.fits\"\n",
    "#g_image = cutout_dir + f\"PS1.{tile_id}.g.fits\"\n",
    "r_image = cutout_dir + f\"CFIS.{tile_id}.r.fits\"\n",
    "#i_image = cutout_dir + f\"PS1.{tile_id}.i.fits\"\n",
    "#z_image = cutout_dir + f\"PS1.{tile_id}.z.fits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [\"CFIS u/\", \"PS1 g/\", \"CFIS r/\", \"PS1 i/\", \"PS1 z/\"]\n",
    "filter_dict = {k:v for v,k in enumerate(filters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confirmed_cutouts():\n",
    "    n_cutouts = 0\n",
    "    for k in list(hf_pos.get(label_subdir).keys()):\n",
    "        f = list(hf_pos.get(label_subdir + k).keys())[0]\n",
    "        img_subgroup = hf_pos.get(label_subdir + k + \"/\" + f + \"/IMAGES\")\n",
    "        n_cutouts += len(img_subgroup)\n",
    "       \n",
    "    confirmed_cutouts = np.zeros((n_cutouts, cutout_size, cutout_size, 5))\n",
    "    n_tiles = len(list(hf_pos.get(label_subdir).keys()))\n",
    "    count = 0\n",
    "    tile_ids = list(hf_pos.get(label_subdir).keys())\n",
    "    for n in range(n_tiles):\n",
    "        tile_id = tile_ids[n]\n",
    "        f = list(hf_pos.get(label_subdir + tile_id).keys())[0]\n",
    "        df = pd.read_csv(label_dir + label_subdir + f + \"/\" + tile_id + \"_labels.csv\")\n",
    "        img_subgroup = hf_pos.get(label_subdir + tile_id + \"/\" + f + \"/IMAGES\")\n",
    "        n_labels = len(img_subgroup)\n",
    "        for i in range(n_labels):\n",
    "            cutout = np.zeros((cutout_size, cutout_size, 5))\n",
    "            dataset_name = tile_id + str(i)\n",
    "            filts = [f + \"/\" for f in list(hf_pos.get(label_subdir + tile_id).keys())]\n",
    "            filt_indices = [filter_dict.get(f) for f in filts]\n",
    "            for (j, ind) in enumerate(filt_indices):\n",
    "                cutout[:,:,ind] = hf_pos.get(label_subdir + tile_id + \"/\" + filts[j] + \"IMAGES/\" + dataset_name)\n",
    "            confirmed_cutouts[count,:,:,:] = cutout\n",
    "            count += 1\n",
    "    return confirmed_cutouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cutout(fits_file, x, y):\n",
    "    cutout = Cutout2D(fits_file[0].data, (x, y), cutout_size, mode=\"partial\", fill_value=0).data\n",
    "    if np.count_nonzero(np.isnan(cutout)) >= 0.05*cutout_size**2 or np.count_nonzero(cutout) == 0: # Don't use this cutout\n",
    "        return None\n",
    "    cutout[np.isnan(cutout)] = 0\n",
    "    lower = np.percentile(cutout, 1)\n",
    "    upper = np.percentile(cutout, 99)\n",
    "    if lower == upper:\n",
    "        cutout_norm = np.zeros((cutout_size, cutout_size))\n",
    "    else:\n",
    "        cutout_norm = (cutout - np.min(cutout)) / (upper - lower)\n",
    "    return cutout_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_cutouts():\n",
    "    n_negative = 6370\n",
    "    u_fits = fits.open(u_image, memmap=True)\n",
    "    #g_fits = fits.open(g_image, memmap=True)\n",
    "    r_fits = fits.open(r_image, memmap=True)\n",
    "    #i_fits = fits.open(i_image, memmap=True)\n",
    "    #z_fits = fits.open(z_image, memmap=True)\n",
    "    cat = Table.read(dest + f\"CFIS.{tile_id}.r.cat\", format=\"ascii.sextractor\")\n",
    "    n = 0\n",
    "    for i in range(len(cat)):\n",
    "        cutout = np.zeros((cutout_size, cutout_size, 5))\n",
    "        if cat[\"FLAGS\"][i] != 0 or cat[\"MAG_AUTO\"][i] >= 99.0 or cat[\"MAGERR_AUTO\"][i] <= 0 or cat[\"MAGERR_AUTO\"][i] >= 1:\n",
    "            continue\n",
    "        x = cat[\"X_IMAGE\"][i]\n",
    "        y = cat[\"Y_IMAGE\"][i]\n",
    "        \n",
    "        u_cutout = create_cutout(u_fits, x, y)\n",
    "        if u_cutout is None:\n",
    "            continue\n",
    "        #g_cutout = create_cutout(g_fits, x, y)\n",
    "        #if g_cutout is None:\n",
    "        #    continue\n",
    "        r_cutout = create_cutout(r_fits, x, y)\n",
    "        if r_cutout is None:\n",
    "            continue\n",
    "        #i_cutout = create_cutout(i_fits, x, y)\n",
    "        #if i_cutout is None:\n",
    "        #    continue\n",
    "        #z_cutout = create_cutout(z_fits, x, y)\n",
    "        #if z_cutout is None:\n",
    "        #    continue\n",
    "        cutout[:,:,0] = u_cutout\n",
    "        #cutout[:,:,1] = g_cutout\n",
    "        cutout[:,:,2] = r_cutout\n",
    "        #cutout[:,:,3] = i_cutout\n",
    "        #cutout[:,:,4] = z_cutout\n",
    "        hf_neg.create_dataset(f\"cutout{n}\", data=cutout)\n",
    "        n += 1\n",
    "        if n == n_negative:\n",
    "            u_fits.close()\n",
    "            #g_fits.close()\n",
    "            r_fits.close()\n",
    "            #i_fits.close()\n",
    "            #z_fits.close()\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout_size = 128\n",
    "confirmed_cutouts = get_confirmed_cutouts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_negative_cutouts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_neg.close()\n",
    "src = os.path.expandvars(\"$SLURM_TMPDIR\") + \"/random_cutouts_cfis.h5\"\n",
    "dest = os.path.expandvars(\"$SCRATCH\") + \"/\"\n",
    "shutil.copy2(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_neg = h5py.File(cutout_dir + \"random_cutouts_cfis.h5\", \"r+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cutouts(pos_start, pos_end, neg_start, neg_end, batch_size):\n",
    "    ratio = (neg_end - neg_start) // (pos_end - pos_start)\n",
    "    b = 0 # counter for batch\n",
    "    cutouts = np.zeros((batch_size, cutout_size, cutout_size, 5))\n",
    "    labels = np.zeros(batch_size)\n",
    "    pos_index = pos_start\n",
    "    neg_index = neg_start\n",
    "    count = 0\n",
    "    while True:\n",
    "        if count > 0 and count % ratio == 0:\n",
    "            cutouts[b,:,:,:] = confirmed_cutouts[pos_index]\n",
    "            labels[b] = 1\n",
    "            pos_index += 1\n",
    "            if pos_index == pos_end:\n",
    "                pos_index = pos_start\n",
    "            b += 1\n",
    "            if b == batch_size:\n",
    "                b = 0\n",
    "                new_shape = cutout_size*cutout_size*5\n",
    "                cutouts_scaled = StandardScaler().fit_transform(cutouts.reshape(batch_size, new_shape))\n",
    "                yield (cutouts_scaled.reshape(cutouts.shape), labels)\n",
    "        else:\n",
    "            cutouts[b,:,:,:] = np.array(hf_neg.get(f\"cutout{neg_index}\"))\n",
    "            labels[b] = 0\n",
    "            neg_index += 1\n",
    "            if neg_index == neg_end:\n",
    "                neg_index = neg_start\n",
    "            b += 1\n",
    "            if b == batch_size:\n",
    "                b = 0\n",
    "                new_shape = cutout_size*cutout_size*5\n",
    "                cutouts_scaled = StandardScaler().fit_transform(cutouts.reshape(batch_size, new_shape))\n",
    "                yield (cutouts_scaled.reshape(cutouts.shape), labels)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, n_epochs, batch_size):\n",
    "    num_cutouts_train_neg = int(0.7*len(hf_neg))\n",
    "    neg_start_train = 0\n",
    "    neg_end_train = num_cutouts_train_neg\n",
    "    neg_start_val = num_cutouts_train_neg\n",
    "    neg_end_val = int(0.9*len(hf_neg))\n",
    "\n",
    "    num_cutouts_train_pos = int(0.7*len(confirmed_cutouts))\n",
    "    pos_start_train = 0\n",
    "    pos_end_train = num_cutouts_train_pos\n",
    "    pos_start_val = num_cutouts_train_pos\n",
    "    pos_end_val = int(0.9*len(confirmed_cutouts))\n",
    "\n",
    "    train_steps = (neg_end_train + pos_end_train) // batch_size\n",
    "    val_steps = ((neg_end_val - neg_start_val) + (pos_end_val - pos_start_val)) // batch_size\n",
    "    neg_weight = (num_cutouts_train_neg + num_cutouts_train_pos) / num_cutouts_train_neg\n",
    "    pos_weight = (num_cutouts_train_neg + num_cutouts_train_pos) / num_cutouts_train_pos\n",
    "    class_weight = {0: neg_weight, 1: pos_weight}\n",
    "    history = model.fit(get_cutouts(pos_start_train, pos_end_train, neg_start_train, neg_end_train, batch_size), \n",
    "                        epochs=n_epochs, steps_per_epoch=train_steps, \n",
    "                        validation_data=get_cutouts(pos_start_val, pos_end_val, neg_start_val, neg_end_val, batch_size), \n",
    "                        validation_steps=val_steps, callbacks=[callback], class_weight=class_weight)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hf_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(encoder):\n",
    "    model = keras.Sequential(encoder)\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(128))\n",
    "    model.add(keras.layers.Dense(64))\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_all(y_true, y_pred):\n",
    "    return keras.losses.MSE(y_true*np.sqrt(weights_all), y_pred*np.sqrt(weights_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = keras.models.load_model(\"../Models/autoencoder_128p\",\n",
    "                                 custom_objects={'custom_loss_all': custom_loss_all})\n",
    "encoder = keras.Model(autoencoder.input, autoencoder.layers[7].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(encoder.layers)):\n",
    "    encoder.layers[i].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout_size = 128\n",
    "classifier = create_classifier(encoder)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "callback = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "classifier.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier = keras.models.load_model(\"../Models/binary_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(classifier, to_file=\"../Models/binary_classifier.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "(classifier, history) = train_classifier(classifier, n_epochs, batch_size)\n",
    "classifier.save(\"../Models/binary_classifier_alt\")\n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "hist_csv_file = '../Histories/history_binary_classifier_alt.csv'\n",
    "with open(hist_csv_file, mode='a') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(history, figname):\n",
    "    plt.plot(history[\"loss\"], color=\"g\", label=\"Training\")\n",
    "    plt.plot(history[\"val_loss\"], color=\"b\", label=\"Validation\")\n",
    "    plt.title(\"Loss Curves for Training/Validation Sets\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"../Loss Curves/\" + figname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history.history, figname=\"binary_classifier_alt.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_negative = int(0.1*len(hf_neg))\n",
    "test_negative = np.zeros((num_negative, cutout_size, cutout_size, 5))\n",
    "test_start = int(0.9*len(hf_neg))\n",
    "test_end = len(hf_neg)\n",
    "i = 0\n",
    "for n in range(test_start, test_end):\n",
    "    test_negative[i] = np.array(hf_neg.get(f\"cutout{n}\"))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_positive = confirmed_cutouts[int(0.9*len(confirmed_cutouts)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cutouts = np.array(list(test_negative) + list(test_positive))\n",
    "test_labels = np.array(list(np.zeros(len(test_negative), dtype=int)) + list(np.ones(len(test_positive), dtype=int)))\n",
    "(test_cutouts, test_labels) = shuffle(test_cutouts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_test, y_test):\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    y_predict = model.predict(x_test)\n",
    "    plt.hist(y_predict)\n",
    "    print(\"Lowest 10 scores:\")\n",
    "    print(sorted(y_predict)[:10])\n",
    "    print()\n",
    "    print(\"Highest 10 scores:\")\n",
    "    print()\n",
    "    print(sorted(y_predict)[-10:])\n",
    "    conf = tf.math.confusion_matrix(y_test, y_predict)\n",
    "    print(f\"Confusion Matrix:\\n {conf}\")\n",
    "    print(\"Test loss: %.3f\" % test_loss)\n",
    "    print(\"Test accuracy: %3f\" % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(classifier, test_cutouts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_pos.close()\n",
    "hf_neg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
